---
title: "truth 4 - time stap"
---

Transcript 

00:00:00 

Who is supposed to be the arbiter of truth between the human and the computer together, do you think? 

00:00:07 

That AI will actually help us. 

00:00:10 

Let me ask you how you think this is going to impact democracies. What are some arguments for and against the future in which humans only have relationships? 

00:00:20 

With AI. 

00:00:22 

What is your worst case fear about? 

00:00:24 

All of this. 

00:00:26 

I'm Ivan mahari. I'm a professor of history at the Hebrew University of Jerusalem and the author of Nexus, A History of information networks from the Stone Age to AI. 

00:00:38 

If you consider, for instance, that we have managed to reach the Moon to split the atom to the cipher DNA, and yet with all our knowledge and wisdom, we are on the verge of destroying ourselves and a traditional answer to this question. 

00:00:58 

Is that there is something wrong in human nature, but the problem is not in our nature. The problem is in our information. 

00:01:09 

If you give good people bad information, they make bad decisions. So why is it that the quality of our information did not improve over thousands of years of history? Why is it that very sophisticated societies in the 20th and 21st century? 

00:01:29 

Have been as susceptible as Stone Age tribes to mass delusion and psychosis, and the rise of destructive ideologies like Stalinism or Nazism. 

00:01:56 

Good. 

00:01:56 

Everybody, thank you for being here. It is a privilege for me to be here with you, Val. I've been a long time fan of his back. When Sapiens was first published, and there is so much to talk about. 

00:02:10 

Between then and now, and how we think about AI and how we think about this new reality, something that I think was actually a theme in your earliest books about reality and truth and information. 

00:02:23 

And in a world where it's going to become all electronic and in a cloud, in the sky, in a whole new way, what it ultimately means when you think about history. So thank you for being. 

00:02:34 

Here we're going to. 

00:02:36 

Talk about so many things, but I'm going to tell you where we want to start tonight. If I could. I want to read you something. This is you, you said. 

00:02:44 

That humankind gains enormous power by building large networks of cooperation. But the way these networks are built. 

00:02:53 

Else predisposes them to use power unwisely. Our problem then is a network problem. You call it, but even more specifically, you say it's an information problem. 

00:03:08 

I want you. 

00:03:09 

To try to unpack that for the audience this evening, because I think that that more than anything else in this book explains, or at least sets the table for what this discussion is all about. 

00:03:23 

So basically the the key question of the book, and of much of human history is if we are so smart, why are we so stupid? 

00:03:34 

That's the key question. 

00:03:36 

I mean, we can, you know, we can reach the moon, we can split the atom, we can create an eye. And yet we are on the verge of destroying ourselves. And not just through one way, like previously. We just said nuclear our war to destroy ourselves. Now we've created an entire menu of ways to destroy ourselves. 

00:03:58 

So. 

00:03:59 

What is happening and you know one basic answer in many mythologies is that there is something wrong with human nature. 

00:04:10 

That we reach for powers that we don't know how to use wisely, that there was really something deeply wrong with us. 

00:04:17 

And the answer that I try to give in the book is that the problem is not in our nature. The problem is in our information. 

00:04:26 

That if you go, if you give good people bad information, they will make bad decisions, self-destructive decisions and this has been happening again and again in history. 

00:04:39 

Because information isn't truth, and information isn't wisdom. The most basic function of information is connection. 

00:04:49 

Information connects many people to a network, and unfortunately the easiest way to connect large numbers of people together is not with the truth, but with fictions and fantasies and delusions. 

00:05:05 

What it? 

00:05:05 

Was going to say there's there's a sense, though, that information was supposed to set us free. 

00:05:09 

Information access to information across the world was the was the thing that was supposed to make us a a better, a better planet. 

00:05:19 

Why would we do that? 

00:05:23 

Well, there was a sense that people in in, in places that didn't have access to communication tools and other things didn't have access to information and didn't have access and maybe to put a point on it to. 

00:05:34 

Good information. 

00:05:36 

You make the argument and now to go back to Sapiens. Frankly, you talk about different types of realities and this is what I think is very interesting because you talk about objective reality. You know, we can go outside and we can see that it's not raining and the sky is blue. And you'd say that's an objective reality, right? 

00:05:55 

But then there are these other kinds of realities that you talk about, which makes us, I think, susceptible to what you're describing. When I say susceptible this idea that we as a group, as humanity that we are storytellers and that we are OK, oddly enough with what you've described as a fictional. 

00:06:15 

Reality. 

00:06:17 

Yeah, I mean. 

00:06:19 

To take an example, suppose we want to build an atom bomb. 

00:06:25 

To do that, suppose that's false. Somebody wants to build an atom bomb. 

00:06:25 

Suppose just. 

00:06:31 

You need to know some facts to build an atom bomb. You need some hold on objective physical reality. If you don't know that E equals MC square. If you don't know the fact of physics, you will not be able to build an atom bomb, but to build an atom bomb you need something else. Besides knowing the fact of physics. 

00:06:53 

You need millions and millions of people. 

00:06:56 

For it. 

00:06:58 

Not just the physicists, but also the miners who mine uranium and the engineers and builders who build the reactor, and the farmers that grow rice and wheat in order to seed all the physicists and engineers and miners and so forth. And how do you get millions of people to cooperate? 

00:07:18 

A project like building an atom bomb, if you just tell them the fact of physics look in equals MC square. Now go to it. Nobody would do it. It's not inspiring. 

00:07:30 

It it doesn't mean anything in this sense of giving motivation. You always need to tell them some kind of fiction fantasy and the people who invent the fictions are far more powerful than the people who know the facts, physics. 

00:07:50 

In Iran today, the nuclear physicists are getting their orders from people who are experts in Shiite. 

00:07:58 

In Israel, increasingly, the physicists in the Soviet Union, they got it from Communist ideologues Germany. 

00:08:12 

It's usually the people who know how to read the story that give orders to the people who merely know. 

00:08:20 

Facts of of of Nuclear Physics and one last point crucial point is that when if you build the bomb and ignore the fact of physics, the bomb will not explode. But if you build a story and ignore the facts of history and biology and whatever, the story still explodes. 

00:08:41 

And usually with a much bigger bank. 

00:08:44 

And so one of the underlying conceits of this book. 

00:08:49 

In my mind, as a reader is that you believe that AI is going to be used for these purposes? 

00:08:54 

Yeah. 

00:08:56 

And that AI ultimately is going to be used as a storyteller to tell untruths and unwise stories, and that those who have the power behind this AI are going to be able to use it in this way. Do you believe it's that people are going to use the AI? 

00:09:15 

In the wrong way, or that the AI is going to use the AI in the wrong way? 

00:09:21 

Initially it will be the. 

00:09:22 

People. But if we are not careful, it will get out of our control and it will be the AI. I mean, maybe we start by saying something about the definition of of AI, because it's now kind of this buzzword which is everywhere and there is so much hype around it that it's becoming difficult to understand what it is. 

00:09:41 

Whenever they try to sell you something, especially in the financial markets, they tell you it's an AI. This is an AI chair and this is an AI table. And this is an AI coffee machine. So buy it. 

00:09:53 

And So what is AI? So if you take a coffee machine as an example, not every automatic machine is an AI, even though it knows how to make coffee. When you press a button, it's not an AI. 

00:10:07 

It's simply doing whatever it was pre programmed by humans to do for something to be an AI, it needs the ability to learn and change by itself. 

00:10:19 

And ultimately to make decisions and create new ideas by itself. So if you approach the coffee machine and the coffee machine on its own accord tells you, hey, good morning, Andrew. I've been watching you for the last couple of weeks. And based on everything I learned about you and your facial expression and then the hour of the day and whatever. 

00:10:41 

I predict that you would like an espresso, so here I made you an espresso. Then it's an AI and it's really an AI. If it says an actually, I've just invented a new drink called best Pretzel, which is even better. I think you would like it better for it took the liberty to make some for you. 

00:11:02 

When it can make decisions and create new ideas by itself. 

00:11:06 

And once you create something like that, then obviously the potential for it to get out of control and to start manipulating you is there and that's the big issue. 

00:11:21 

And so how far away do you think we are from that? 

00:11:28 

When Openai developed GPT 4 like two years ago, they did all kinds of tests. What can this thing do? So one of the tests they gave are GPD for was to solve capture puzzles that. 

00:11:44 

When you access. 

00:11:45 

A web page, a bank, or something you may want to know. 

00:11:48 

You're a human being, or you're a bot, so you are. You have these twisted letters and symbols. 

00:11:53 

And this is or the 9 squares and it says which one has something with four legs in it and you see the animal and click the animal. 

00:11:54 

Yeah. 

00:11:59 

All these kinds of things. 

00:12:00 

Yeah. 

00:12:02 

And GPT 4 couldn't solve it. But GPD 4 access. 

00:12:07 

Task Rabbit, which is an online site where you can hire people to do. 

00:12:13 

All kinds of things for you. 

00:12:15 

And you know, these captures were developed so humans can solve them. But AI, I still struggle with. 

00:12:22 

So it wanted to hire a human. That's interesting. The human got suspicious. 

00:12:31 

The human asked why do you need somebody to solve the culture? Puzzles are. 

00:12:36 

You a robot? 

00:12:39 

And at that point, GPT 4 said no, I'm not a robot. I have a vision impairment, which is why I have difficulty with these captures. 

00:12:49 

So and how to lie? 

00:12:51 

It it had basically theory of mind. On some level it it it knew what you know. There's so many different explanations. 

00:12:59 

And lies you. 

00:13:00 

Can tell, he told a very, very effective lie. 

00:13:03 

It's the kind of life that occupation improve, but I'm not going to ask any more questions about it. I will just take the word of GPT for for it. 

00:13:14 

And nobody told it to lie, and nobody told it. Which lie will be most effective? It landed. 

00:13:22 

The same way that humans and animals learn things by interacting. 

00:13:27 

With the world. 

00:13:30 

Let me ask you a different. 

00:13:31 

Question. To the extent that. 

00:13:33 

You believe that the computer will be able to do this to us. 

00:13:38 

Will with the. 

00:13:38 

Help of the computer we be able to avoid these problems on the other side, meaning there's a cat and mouse game here and and the question is between the human and the computer together, will we be able to be more powerful and maybe power or is good or bad but be able to get at truth? 

00:13:46 

Then. 

00:14:00 

In a better way. 

00:14:02 

Potentially yes. I mean the the point of talking about it, of writing a book about it is that it's not too late and that also the AI's are not inherently evil or malevolent. It's a question of how we shape this technology. We still have the power. 

00:14:20 

We are still at the present moment. We are still far more powerful than the AIS. They in very narrow domains like playing chess or playing golf. They are becoming more intelligent than us, but we still have a huge edge. Nobody knows for how many more years, so we need to use it and make sure that we direct the development of this technology. 

00:14:43 

Are in in a safe. 

00:14:44 

Direction. OK, so. 

00:14:45 

I'm going to make you king for the day. 

00:14:48 

What would you do? How would you? Would you regulate these things? How what? What is it exactly in? 

00:14:53 

In an environment. 

00:14:54 

And world where we don't have global regulators, we don't have people who like to interact and even talk to each other anymore. Frankly, where we've talked about regulating social media, at least in the United States for the past 15 years now. 

00:15:08 

And have done next to nothing. 

00:15:11 

How we would get there, what that looks like? 

00:15:14 

You know, there are many regulations we should enact right now and we can talk about that. One key regulation is make social media corporations liable for the actions of their algorithms, not for the actions of their users and for the. 

00:15:30 

Sections of the the corporate algorithms and another key regulation that AIS cannot pretend to be human beings. They can interact with us only if they identify as AIS. But beyond all these specific regulations, the key thing is institutions. 

00:15:51 

Nobody is able to predict. 

00:15:54 

How this is going to play out, how this is going to develop in the next 5-10, fifty years? So any any thought that we can now kind of regulate in advance it's it's impossible. No rigid regulation will be able to deal with what we are facing. We need living institutions. 

00:16:15 

That have the best human talent and also the best technology. 

00:16:19 

That can identify problems as they develop and react on the fly, and we are living in an era in which there is growing hostility towards institutions. But what we've learned again and again throughout history, is that all institutions are able to deal with these things, not single. 

00:16:39 

Individuals and not some kind of miracle regulations. 

00:16:44 

Play it out. What is the the and and you play it out in the book a little bit when it comes to dictators and your real fears. What is the your worst case fear about? 

00:16:45 

OK. 

00:16:55 

All of this. 

00:16:58 

One of the problems with AI is that there isn't just like this single nightmare scenario with nuclear weapons. One of the good things about nuclear weapons it it was very easy for everybody to understand the danger, because there was basically one extremely dangerous scenario. 

00:17:18 

Nuclear war. 

00:17:20 

With AIS, because they are, they are not tools like atom bombs, they are agents. 

00:17:27 

That that's the most important thing to understand about them. An atom bomb, ultimately, is a tool in our hand. All decisions about how to use it and how to develop it. Humans make these decisions. 

00:17:40 

AI is not a tool in our hand. It's an autonomous agent. 

00:17:46 

That, as I said, can make decisions and can invent ideas by itself, and therefore by definition, there are hundreds of dangerous scenarios that are many of them we cannot predict because they are coming from a non human intelligence. 

00:18:06 

But think differently than us. 

00:18:10 

You know the the acronym AI. It traditionally stood for artificial intelligence, but I think it should stand for alien intelligence. Not in the sense that coming from outer space in the sense that it's an alien kind of intelligence, it thinks. 

00:18:26 

Differently than a human being, and it's not artificial because an artifact, again, is something that we create and. 

00:18:34 

Small and this is now as the potential to go way beyond what we can predict and control still from all the different scenarios, the one which I think are speaks to to my deepest fears and also to to humanity's deepest fears is. 

00:18:54 

Being trapped. 

00:18:56 

In a world of delusions created by AI, you know the fear of robots rebelling against humans and shooting people and whatever. This is very new. This goes back a couple of decades, maybe to the mid 20th. 

00:19:13 

But for thousands of years, humans have been haunted by a much deeper fear. We know that our societies are ultimately based on stories. 

00:19:24 

And there was always the fear that we will just be trapped inside the world of illusions and delusions and fantasies, which we mistake for reality. 

00:19:37 

And will not be able to escape from, but will lose all touch with truth with reality. 

00:19:43 

Part of that exists today. 

00:19:44 

Yeah. And, you know, it goes back to the parable of The Cave in Platos to losophy of a group of prisoners chained in a cave facing a blank wall, a screen. 

00:19:59 

And there are shadows projected on this screen and they mistake the shadows for reality. And you have the same fears in ancient Hindu and Buddhist philosophy of the world of Maya. The world of illusions. And AI is kind of the perfect tool. 

00:20:18 

To create it. 

00:20:19 

We are already seeing it being created all around. 

00:20:23 

If the main metaphor of the early Internet age was the web, the World Wide Web and the web connects everything. So now the main metaphor is the cocoon. It's the web which closes on us, and I live inside 1 information. 

00:20:43 

And you live inside maybe a different information cocoon and there is absolutely no way for us to communicate because we live in separate different reality. 

00:20:55 

Let me ask you how you think this is going to impact democracies, and let me read you something from the book because I think it speaks to maybe even this election here in the United States, the definition of democracy as a distributed information network with strong self correcting mechanisms stands in sharp contrast to a common misconception. 

00:21:15 

That equates democracy only with elections. 

00:21:20 

Elections are a central part of the Democratic toolkit, but they are not democracy. In the absence of additional self correcting mechanisms, elections can easily be rigged. Even if the elections are completely free and fair by itself, this too doesn't guarantee democracy for democracy is not the same thing as. And this one's interesting. 

00:21:41 

Majority dictatorship. 

00:21:44 

What do you? 

00:21:44 

Bye, Matt. 

00:21:46 

What many things, but I mean on the most basic level, just so in Venezuela that you can hold elections and if the dictator has the power to rig the elections, so it means nothing. 

00:22:01 

The great advantage of democracy is that it corrects itself. 

00:22:06 

It has this mechanism that you try something, but people try something. They vote for a party for a leader. Let's try this bunch of policies for a couple of years. If it doesn't work well, we can now try something else. 

00:22:20 

That the the basic ability to say we made the mistake, let's try something else. This is democracy. In putting Russia, you cannot say that we made a mistake. Let's let's try somebody else in democracy. You can. But there is a problem here. You give so much power for limited time. You give so much power. 

00:22:41 

To a leader, to a party. 

00:22:44 

To enact certain policies. But then what happens is that leader or party utilizes the power that the people gave them not to enact a specific time limited policy, but to gain even more power and to make it impossible to get rid of them anymore. 

00:23:03 

It was always one of the key problems of democracy that you can use democracy to gain power and then use the power you have to destroy democracy. Everyone said it in in Turkey, I think best that, he said. Democracy is like a tram, like a. 

00:23:20 

Train you take it until you reach the destination, then you go down. You don't stay on the train. 

00:23:31 

Ah. 

00:23:37 

And. 

00:23:39 

For most of history, large scale democracy was really impossible because democracy in essence is a conversation between a large number of people. 

00:23:52 

And in the ancient world, you could have this conversation in a tribe, in a village, in a city state like Athens or Republican Rome. But you could not have it in a large Kingdom with millions of people because there was no technical means to hold the conversation. So we don't know of any example. 

00:24:13 

Of a large scale democracy before the rise of modern information technology in the late modern age, so democracy is built on top of information tech. 

00:24:24 

Logic and when there is a big change in information technology like what is happening right now in the world, there was an earthquake. 

00:24:34 

In the structure built on top of it. 

00:24:36 

Who was supposed to be the arbiter of truth in these democracies? And the reason I raised this issue is we we have a big debate going on in this country right now about free speech and how much information we're supposed to know what access to information we're supposed to get. 

00:24:47 

Yeah. 

00:24:54 

Mark Zuckerberg just came out a couple weeks ago and said that, you know, there was information during the pandemic that he was suppressing that in retrospect, he wishes now he wasn't suppressing. 

00:25:06 

Mm-hmm. 

00:25:07 

This gets to the idea of truth, and it also gets to the idea of what information you said. Information is not. 

00:25:12 

Truth, yeah. 

00:25:14 

Are we supposed to get access to all of it and decipher what's real ourselves? Is somebody else supposed to do it for us? And will AI? 

00:25:23 

Ultimately, be that arbiter. 

00:25:26 

So there are a couple of of of of different questions here, one about free speech and the other about who is supposed to most important thing is that free speech includes the freedom to tell lies, to tell fictions, to tell fantasies. So it's not the same as the question of truth. 

00:25:47 

Ideally, in the democracy you also have the. 

00:25:49 

Freedom to lie. 

00:25:51 

You also have the freedom to spread directions. 

00:25:53 

The ideal is why is that ideal? 

00:26:00 

And and this is something that is crucial to understand, because the tech giants, they are constantly confusing the question of truth with the question of free speech. And there are two different questions. 

00:26:14 

We don't want a kind of truth police that constantly tells human beings what they can and cannot say. 

00:26:23 

There are, of course limits even to that, but ideally yes, people should be able to also tell fictions and fantasies and so forth. 

00:26:34 

This is information, it's not true. 

00:26:37 

The crucial role. 

00:26:40 

Of this feeling out of this ocean of information. 

00:26:44 

The rare and costly kind of information, which is truth. 

00:26:50 

This is the role of several different, very important institutions in society. This is the role of scientists. This is the role of journalists. This is the role of judges. 

00:27:02 

And the role or the aim of these institutions is not to limit the freedom of speech of the of the people, but to tell us what is the truth. Now the problem we now face is that there is a sustained. 

00:27:22 

Attack. 

00:27:23 

On these institutions and on the very notion that there is truth because the dominant worldview in large parts of both the left and the right on the left takes a mouse's shape, and on the right the populist shape. 

00:27:42 

They tell us that the only reality is power. 

00:27:49 

All, all the world, all the universe, all reality is just power. The only thing humans want humans want is power and any human interaction like we have in this conversation is just a power struggle. And in every such interaction, when somebody tells you something. 

00:28:10 

The question to ask, and this is what Donald Trump meets Karl Marx, and they shake hands. They agree on that when somebody tells you something, the question to ask is not what is. Is it true? There is no such thing. The question to ask is who is winning and who is losing? 

00:28:29 

Whose privileges like what I have just said, whose privileges are being served by. 

00:28:36 

And the idea is that when scientists, journalists, judges, people at large, they are only in pursuing power. And this is an extremely cynical and destructive view which undermines all institutions. 

00:28:55 

Luckily, it's not true. 

00:28:59 

Especially when we look at ourselves, if we start to understand humanity just by looking at ourselves, most people will say yes, I want some power in life, but it's not the only thing I want. I actually want truth. Also, I want to know the truth about myself, about my life. 

00:29:19 

The truth about the world, and the reason that there is this deep yearning for truth, is that you can't really be happy. 

00:29:28 

If you don't know the truth about yourself and about your life, if you look at these, people are obsessed with power. People like Vladimir Putin or Benjamin Netanyahu or Donald Trump, they have a lot of power. They don't seem to be particularly happy. 

00:29:47 

And when you realize that I'm a human being and I I really want to know at least some truth about life, why isn't it true also of others? 

00:29:59 

And yes, there are problems in institutions. There are, there is corruption, there is influence. And so. But This is why we have a lot of different institutions. 

00:30:08 

I'm going to throw another name at you. You talked about Putin, you talked about Trump, you talked about Netanyahu. He's not a politician, but he might want to be some. 

00:30:16 

Point. 

00:30:17 

Elon Musk. 

00:30:18 

Hmm. 

00:30:20 

Where do you put him in this? And the reason I ask? Talking about truth and information, he's somebody who said that he wants to set all information free because he believes that if the information is free, he says that you, all of us, will be able to find and get to the. 

00:30:36 

Truth, there are others. 

00:30:37 

MHM. 

00:30:38 

Who believe that all of that information. 

00:30:40 

It will obscure the truth. 

00:30:42 

I think that it's a very naive view of information and truth. The idea that you just open the floodgates and let information flood the world and the truth will somehow rise to the top. 

00:30:57 

You don't know anything about history. If this is what? 

00:31:00 

You think? 

00:31:03 

It just doesn't work like that. The truth? Again, it's costly. It's well, like, if you want to write a true story in the newspaper or an academic paper or whatever, you have to research so much. It's very difficult. 

00:31:19 

Takes a lot of time, energy, money. If you want to create some fiction, it's the easiest thing in the world. 

00:31:26 

Similarly, the truth is often very complicated because reality is very complicated, whereas fiction and fantasy can be made as simple as possible, and people in most cases they like simple stories and the truth is often painful. Whether you think about and the truth about entire countries. 

00:31:41 

Right. 

00:31:47 

Or the truth about individuals. The truth is often unattractive. It's painful. We don't want to know. For instance, the pain that we inflict sometimes on people in our life or on ourselves. 

00:32:03 

Because the truth is, is costly and it's complicated and it can be painful in a completely kind of free fight. It will lose. This is why we need institutions. 

00:32:16 

Like newspapers or like academic institutions, and once again something very important about the tech giants. 

00:32:25 

My problem with Facebook and and and Twitter and so forth. I don't expect them to censor their users. We have to be very, very careful about censoring the expression of real human beings. 

00:32:41 

My problem is the algorithms. Yes, even the even when they're not telling the truth, we're very careful about censoring the the the, the free expression of human beings. My problem is with the algorithms. 

00:32:42 

Even when they're telling the truth, even when they're telling untruths. 

00:32:57 

That if you look at the historical case, like the massacre of the Rohingya in Myanmar in 20/16/2017, in which thousands of of people were murdered, 10s of thousands were raped, and hundreds of thousands were expelled and are still refugees in Bangladesh and elsewhere. 

00:33:18 

This ethnic cleansing campaign was fueled by a propaganda campaign that are generated intense hatred among Buddhists, bunnies in Myanmar against the Rohingya. 

00:33:33 

Now this campaign to a large extent took place on Facebook. 

00:33:40 

And the Facebook algorithms played a very central role in it. Now, whenever these accusations have been made and they have been made by amnesty in the United Nations and so forth. 

00:33:52 

Facebook basically said, but we don't want to censor the free expression of people that all these conspiracy theories about the Rohingya, that they are all terrorists and they want to destroy us. They were created by human beings. 

00:34:07 

And who are we? Facebook. The fans of them. And the problem there is that the role of the algorithms. 

00:34:17 

Was at that point was not in creating the stories, it was in disseminating in spreading particular stories, because people in Myanmar were creating a lot of different kinds of content. In 20/16/2017, the war hate filled conspiracy theories, but there are also sermons on compassion. 

00:34:40 

In cooking lessons and biology lessons and so much content. 

00:34:44 

And in the battle between for human attention, what will get the attention of the users in Myanmar, the algorithms of the Kingmakers and the algorithms were given by Facebook. 

00:35:00 

A clear and simple goal. Increase user engagement. 

00:35:07 

Keep more people, more time on the platform because this is the basis for once still is for, for Facebook. 

00:35:15 

'S business model. 

00:35:17 

Now the algorithms and this goes back to AIS that make decisions by themselves and invent ideas by themselves. Nobody in Facebook wanted them to be an ethnic cleansing campaign. 

00:35:30 

Most of them didn't know anything about Myanmar and what was happening there. They just told the algorithms, increased user engagement, and the algorithms experimented on millions of human Guinea pigs. And they discovered that if you press the hate button and the seal but. 

00:35:47 

In the human mind, you keep that human glued to the screen, so they deliberately started spreading hate filled conspiracy theories and not. 

00:35:59 

Cooking lessons or summers on on compassion and this is the expectation, don't censor. 

00:36:06 

The users, but if your algorithms deliberately spread hate and fear because this is your business model, it's on you. This is your fault. 

00:36:18 

OK. But I'm gonna make it more complicated. I'm going to make it more complicated. And the reason we we're talking about Facebook and and Elon Musk. 

00:36:25 

Is that the algorithms that are effectively building these new AI agents are scraping all of this information. 

00:36:36 

Off of these sites, this human user generated content which we've decided everybody should be able to see, except once the AI's see it, they will see the conspiracy theories. They will see the misinformation and the question is how the AI agent will ever learn what's real and what's not. 

00:36:57 

That is a very, very old question that you know the editor of. 

00:37:02 

The New York. 

00:37:03 

Times the editor of the Wall Street Journal have dealt with this question before. Why do we need to start again? As if there has never been any history? 

00:37:14 

Like the question, there is so much information out there. How should I know what's true or not? 

00:37:20 

People have been there before. If you run Twitter or Facebook, you are running one of the biggest media companies in the world, so take a look at what media companies have been dealing with in the previous generations and previous centuries. It's basic. 

00:37:35 

They should be liable for what's on their sites right now. 

00:37:39 

The the tech companies. 

00:37:40 

The tech companies should be liable for for the user generated content. 

00:37:42 

They should be. 

00:37:43 

Liable. No, they should be liable for the actions of their algorithms. 

00:37:48 

If their algorithms decided to put at the top of the news feed a hate filled conspiracy theory, it's on them not on the person who created the conspiracy theory in the 1st place. The same way that you know, like the chief editor of the New York Times, designed to put a hateful conspiracy theory at the top of the of the first page of the New York Times. 

00:38:10 

And when you come and and tell them what have you done, they say I haven't done anything. I didn't write it. I just decided to put it on the front page of the New York Times. That's all. That's not. That's a huge thing. Editors have a lot more power than the authors to create the. 

00:38:30 

Very cheap to create content. The really key point is what gets the attention. You know, I'll give you an example from thousands of years ago. 

00:38:40 

That when Christianity began. 

00:38:43 

There were so many. 

00:38:44 

Stories circulating about Jesus and about the disciples and about the Saint. Anybody could invent a story in that, Jesus said. 

00:38:53 

And there was no Bible. 

00:38:56 

There was no New Testament in the time of Jesus or 100 years after Jesus or even 200 years after Jesus. 

00:39:02 

There are lots and lots of stories and texts and parable. At a certain point. 

00:39:09 

The leaders of the Christian Church in the 4th century, they said this cannot go on like this. We have to have some order in this flood of information. How would Christians know what information to value, which texts to read and what are forgeries of whatever that they should just ignore? 

00:39:29 

And their hands are two church councils in Hippo and in Carthage today in Tunisia in the late 4th century. And this is where they they didn't write the texts. The texts were written generations previously, but so many different people. 

00:39:47 

They decided what will get into the Bible and what will stay out. The book didn't come down from heaven in its coming complete fall. 

00:39:58 

There was a committee that decided which of all the texts will get in and, for instance, they decided that are a text which claimed to be written by Saint Paul. But many scholars today believe that it wasn't written by Saint Paul. The First Epistle to Timothy. 

00:40:19 

Which was a very misogynistic text which said basically the role of women is to be silent and to build children. This got into the Bible, whereas another text, the acts of Paul and Thecla, which portrayed Sheckler as a disciple of Saint Paul, are preaching and leading the Christian. 

00:40:40 

Community and doing miracles and so forth. They said. Nah, let's leave that one out. 

00:40:46 

And the views of billions of Christians for almost 2000 years. 

00:40:53 

About women and their capacities and their role in the church in the community, it was decided and not by the authors of First Timothy and the Act of politics. 

00:41:08 

But by this church committee, who sat in what is today Tunisia and went over all these things and decided this will be in, and this will be out, this is enormous power. The power of curation, the editorial power, you know, you think about the the role of newspapers in, in, in, in modern politics. 

00:41:29 

It's the editors who have the most power learning before he was dictator of the Soviet Union is one job, was chief editor of Isra. 

00:41:39 

And Mussolini also first was a journalist when he was editor. 

00:41:44 

Immense power and now the editors increasingly are the algorithms. You know, I'm here partly on. 

00:41:51 

The book too. 

00:41:53 

And I know that my number one customer. 

00:41:56 

Is is the algorithm? 

00:41:58 

Like if I can get the algorithm to recommend my book, the humans will follow. 

00:42:06 

So is it an immense power? 

00:42:09 

And. 

00:42:11 

That's above my pay grade. I mean, I'm just sitting here on the stage talking with you. 

00:42:19 

We'll do this. We'll do that, we'll. 

00:42:21 

Put it like this. 

00:42:25 

Realize the immense power of the recommendation algorithm. The editor this comes with responsibility. 

00:42:32 

So then I don't think that we should hold Twitter or Facebook responsible for what the users, right. And we should be very careful about censoring users even when they say when they write lines. But we should hold the companies responsible for the choices. 

00:42:53 

And the decisions of their algorithms. 

00:42:55 

You talked now about Lenin, and we've talked about all sorts of dictators. One of the things. 

00:42:59 

You talk about in the book is the prospect. 

00:43:02 

That AI could be used to turn a democracy into totalitarian state? Yeah, I thought was fascinating. 

00:43:11 

Because it would really require AI to cocoon all of us in a unique way, telling some kind of story that captures an entire country. How would that work in your mind? 

00:43:25 

And then we don't see reality. We see the information. 

00:43:30 

That we are exposed. 

00:43:32 

And if you have better technology to to create and to control information, you have a much more powerful technology to control humans. 

00:43:45 

Previously, there was always limitations to how much a central authority could control what people see and think and do. Even if you think about the most totalitarian regimes of the 20th century. 

00:44:01 

Like the US saw or Nazi Germany, they couldn't really follow everybody all the time. It was technically impossible. Like if you have 200 million Soviet citizens to follow them around all the time, 24 hours a day, you need about 400 million KGB. 

00:44:21 

Agents because even KGB agents need to sleep. Sometimes they need to eat. Sometimes they need 2 shifts. So 200 million citizens. You need 400 million. 

00:44:29 

Agents you don't. 

00:44:29 

Have 400 million KGB agents, even if you. 

00:44:33 

You still have a bigger problem because let's say that two agents follow each citizen 24 hours a day. What are they doing? The end, they write a report. This is like 1940 or 1960. It's paper. They write a paper report. So every day KGB headquarters in Moscow is flooded. 

00:44:52 

With 200 million reports about each citizen where they went, what they read, who they meant, somebody needs to analyze all the information they where do they get the analysts? It's absolutely impossible, which is why even in the Soviet Union. 

00:45:10 

Privacy will feel the default. 

00:45:13 

You never knew who is watching and who is listening, but most of the time nobody was watching and listening. And even if they were, most likely the report about you would be buried in some place in the archives of the KGB. Nobody would ever read it. 

00:45:28 

Now both problems of surveillance are solved by AI to follow all the people of the country 24 hours a day, you do not need millions of human agents. You have all these digital agents, the smartphones, the computers, the microphones and so forth. 

00:45:47 

You can put the whole population under 24 hour surveillance and you don't need human analysts to analyze all the information you have. 

00:45:56 

AI INS to do it? 

00:45:57 

What do you make of the idea that an entire generation has given up on? 

00:46:02 

The idea of privacy. 

00:46:05 

Right. We all put our pictures on Instagram and Facebook and all of these sites we hear about a security hack at some website and we go back the next day and try to buy and put our credit card on the same site again. We say that we're very anxious about privacy. We we. 

00:46:23 

We love to. 

00:46:23 

Tell people how anxious we are about the. 

00:46:26 

And then we do things that are the complete opposite. 

00:46:30 

Because there is. 

00:46:30 

Immense pressure on us to do them. 

00:46:34 

Part of it is is despair. 

00:46:37 

And people still cherish privacy and part of the problem is that we haven't seen the consequences yet yet yet we will see them quite soon. It's like this huge experiment conducted on billions of of human Guinea pigs, but we still haven't seen the consequences of this. 

00:46:57 

Annihilation. 

00:46:59 

Of privacy and it is extremely, extremely dangerous. Again, going back to the freedom of speech issue, part of this crisis of freedom of speech is the erosion of the difference between private and public. 

00:47:15 

That there is a big difference. If there's just the two of us are sitting alone somewhere talking just you and me. Or if there is an entire audience and it's public now, I'm a very big believer in the right to stupidity. People have a right to be stupid in private. 

00:47:35 

But when you talk in private with your best friends, with your family, whatever you have a right to be really stupid, to be offensive as a gay person, I would say that even politicians, if they tell homophobic jokes in a private situation with their friends, this is none of my business. 

00:47:55 

And it's actually good that politicians would have situations where they can just relax and say the first stupid thing that pops up in the. 

00:48:06 

This should not happen in public. 

00:48:09 

Now it's going to be entered into the. 

00:48:09 

1. 

00:48:11 

Algorithm. Yes, and this inability. 

00:48:17 

To know what that anything they say, even in private, cannot go viral. So there is no difference, and this is extremely harmful. Part of what's happening now in the world is this kind of tension. 

00:48:32 

Between organic animals, we are organic animals and an inorganic digital system which is increasingly controlling and shaping the entire world. Now part of being an organic entity is that you live by cycles, day and night. 

00:48:52 

Winter and summer growth and decay. Sometimes you are active, sometimes you need to relax and to rest. 

00:49:00 

Are algorithms and AIS and computers? They are not organic, they never need rest. They are on all the time. And the big question is whether we adapt to them or they adapt to us and and more and more. Of course we have to adapt to them. We have to be on all the time. 

00:49:20 

The new cycle is always on and everything we see, even when we are supposedly relaxing with friends, it can be public. So the whole of life becomes like this one long job. 

00:49:33 

Any stupid thing you did in some college when you are 18, it can meet you down the road. 

00:49:42 

And this is destructive. 

00:49:44 

To how we function. We even think about the market like Wall Street. As far as I know, it's open Monday to Friday is 9:30 to 4:00 in the afternoon, likely from Friday at 5 minutes past four, a new war erupted in the Middle East. The market will react only on Monday. 

00:50:03 

It is still running by organic cycles now. What would happen to human bankers and finance citizens? What is happening when the market is always active? You can never relax. 

00:50:15 

We've got some. 

00:50:16 

Questions I think that are going to come out here on, on cards, which I want to ask in just a moment. I have a couple of questions before we get to them. One is, have you talked to folks like Sam Altman who runs open AI or the folks at Microsoft and Know Bill Gates was a big fan of your books in the past or the folks at Google? 

00:50:34 

What do they say? 

00:50:35 

When you discuss this with them, and do you trust them as humans when you've met them, do you go? I trust you, Sam Altman. 

00:50:45 

Most of them are afraid they understand. 

00:50:48 

Afraid of you or afraid of AI? 

00:50:50 

Afraid of what? What they are doing? Afraid of what is happening. They understand better than anybody else their potential, including the destructive potential of what they are creating. 

00:51:02 

And they are very afraid of it. At the same time, their basic stick is that I'm a good guy and I'm very concerned about it. Now you have these other guys, they are bad. They don't have the same kind of responsibilities that I have. So it would be very bad for humanity if they create it first. So I must be the one. 

00:51:23 

Operated first and you can trust me that I will know. I will at least do my best. 

00:51:29 

To to to keep it under and everybody else saying it. And I think that to to some extent they are genuine about it. There is of course also this. 

00:51:39 

Another element in in there of extreme kind of pride and hubris, but they are doing the most important thing in basically not just the history of humanity, the history of life. 

00:51:52 

Do you think they are? 

00:51:54 

They could be, yes, if you think about the timeline of the universe, at least as far as. 

00:51:59 

We know it. 

00:52:01 

So you have basically two stops. First, stop 4 billion years ago, the first organic life forms in merge on Planet Earth, and then for 4 billion years nothing major happens. Like for four million years. It's more of the same. It's more organic stuff. So you have amoebas and you have dinosaurs and you have. 

00:52:23 

Humans, but it's all organic. 

00:52:25 

And then here comes Elon Musk, or some Altman, and that's the second important thing in the history of the universe. The beginning of inorganic evolution. 

00:52:36 

Because AI is just the very, very beginning of its evolutionary process, is basically like 10 years or 15 years old. We haven't seen anything yet deep 4:00 and all these things, they are the amoebas of AI evolution. 

00:52:56 

And who knows how the AI dinosaurs are going to look like, but the the name. 

00:53:04 

On the inflection point of the history of the universe, if that name is Elon Musk or that name is sent out, one that's a big thing. 

00:53:15 

We've got a a bunch of really great questions and actually one of them is where I wanted to go before we even got to these. So we're just go straight to it. We're gonna make. It's actually a bit of a right turn of a question given the conversation, though, you mentioned that Netanyahu on point. So the question here on the card says you're Israeli. 

00:53:33 

Do you really think that the Israel Palestinian conflict is solvable? 

00:53:38 

And I thought actually that was an important question because it actually touches on some of these larger issues that you've raised about humanity. 

00:53:46 

So on on one level, absolutely yes, it's not like one of these kind of mathematical problems that we have and mathematical proof that there is no solution to this problem. 

00:53:59 

No, there are all solutions to the Israeli Palestinian conflict. It is because it's not a conflict about objective reality. 

00:54:07 

It's not a conflict about land or food or resources that you say, OK, there is just not enough food. Somebody have to starve to death or there is not enough land. Somebody has to be thrown in. 

00:54:20 

See, this is not the case. There is enough food between the Mediterranean and Jordan to feed everybody. There is enough energy. There is even enough land. Yes, it's a very crowded place, but technically there is enough space to build houses and synagogues and mosques and factories and hospitals and schools for everybody. So there is no objective. 

00:54:42 

Voltage. But you have people each in their own information cocoon, each with their own mass delusion, each with their own fantasy, are basically denying either the existence of the other side or the right of the other side to exist. 

00:55:01 

And the war is basically an attempt. 

00:55:06 

To make the other side disappear, like my mind, there's no space in it. It's not that the land has no space for the people. My mind has no space in it for the other people. So I will try to make them disappear the same way they don't exist in my mind. They also must not exist. 

00:55:27 

In reality. 

00:55:28 

And this is on both sides. 

00:55:31 

And again, it's not an objective problem. 

00:55:34 

It's a problem of what is inside the minds of people, and therefore there there is a solution to it. Unfortunately, there is no motivation. 

00:55:43 

What is the solution? 

00:55:45 

For someone who grew up there who's lived there? 

00:55:48 

You're. You're not there right now. 

00:55:49 

Right, I'm here, right? 

00:55:50 

Now you're here right now. 

00:55:51 

Yeah. 

00:55:54 

No. 

00:55:55 

Again, if you go back, say, to the two state solution, it's it's completely workable solution in objective terms. 

00:56:05 

And you can divide the land and you can divide the resources so that side by side you have a Palestinian state and you have an Israel and Israel and they are both. They both exist and they are both viable and they both provide security and prosperity and dignity to their citizens, to their inhabitants. 

00:56:24 

How how would you? How would you do that though, because part of the part of the issue is is a chicken and egg issue here, it seems like where the Israelis or Netanyahu would say, look, unless we are fully secure and we feel completely good about our security, we can't really even. 

00:56:25 

What? 

00:56:41 

Entertain a conversation just about anything else, right? And the. 

00:56:45 

Palestinians on the. 

00:56:46 

Other side effectively say the opposite, which is to say that we need. 

00:56:50 

You you need to. 

00:56:50 

Solve our our our issues here. 

00:56:52 

Both sides of the problems both sides are right. Each side thinks that the other side is trying to annihilate it, and both are right. This is the problem. 

00:57:02 

And both are right, but tell them that's. 

00:57:04 

A problem if both are right. 

00:57:11 

Our minds it's very difficult to change the minds of other people, the first crucial. 

00:57:16 

That is to say, these are the people. They exist, and they have the right to exist. 

00:57:25 

The issue is that both sides suspect that they don't think that we should exist any compromise that they would make is just because they are now a bit weak. 

00:57:38 

So they will are willing to compromise on something, but deep in their hearts they think we should not exist, so that sooner or later, when they are stronger, when they have the opportunity, they will destroy us. And again, this is correct. This is what is in the hearts and minds of both sides. So the place. 

00:57:58 

Start is to you know inside first our heads. 

00:58:04 

Come to recognize that the other side exists and it has a right to exist that even if someday will have the power to completely annihilate them, we shouldn't do it because they have a right to exist. And this is something. 

00:58:18 

We can do for us. 

00:58:18 

OK, let me ask you, let me ask you a question. 

00:58:21 

I think. 

00:58:24 

People in Israel would say that people in Palestine have a right to exist. 

00:58:28 

Yeah. 

00:58:29 

There is a you don't believe that you don't. 

00:58:31 

You think that's not? 

00:58:32 

The case, unfortunately for a significant percentage of the citizens of Israel and certainly of the members of the present governing coalition, this is not the case. 

00:58:44 

And you think they have no right to exist? Because I was going to say they would say that they feel that the Palestinians. 

00:58:46 

They. 

00:58:51 

Want to annihilate them and do you? 

00:58:54 

Believe that yes, they. And then they think that the Palestinians want to annihilate us and they are right. They do want to annihilate us. But also again, I would say I don't have the numbers to give you, but a significant part of the Israeli public and a very significant part of the. 

00:59:12 

Current ruling coalition in Israel, they want to drive the Palestinian completely from from the land. 

00:59:20 

And maybe they say we are now too weak, so we can't do it. We have to compromise them only to do a little bit. But ultimately this is what they really think and want and at least some members of the coalition are completely open about it and their messianic fantasy. They actually want a bigger and bigger. 

00:59:41 

War. They want to set the entire Middle East on fire. 

00:59:45 

Because they think that when the smoke would clear, yes, there will be hundreds of thousands of casualties. It will be very difficult. It will be terrible, but in the end we'll have the entire land to ourselves and there will not be any more Palestinians between the Mediterranean and and the Jordan. 

01:00:03 

Let me ask you one other very political question as it relates to this and actually we have a great segue back into our conversation about AI as it happens. 

01:00:12 

Politically here in the US, we have an election. 

01:00:15 

MHM. 

01:00:16 

And there's some very interesting questions about how former President Trump, if he were to become the president, would be good or bad for Israel. I think there's a perception that he would be good for Israel. 

01:00:28 

You. 

01:00:28 

May disagree and what you think, Vice President Harris, if she was the president, would be good or bad for Israel, or good or bad. 

01:00:37 

For the Palestinians. 

01:00:39 

How do you see that? 

01:00:40 

And I I can't predict what each will decide, but it's very clear that President Trump is undermining the global order. 

01:00:52 

He's in favor of chaos. 

01:00:54 

He is in favor of destroying the liberal global order that we had for the previous couple of decades and which provided the with all the problems with all the difficulties, the most peaceful era in human history. 

01:01:14 

And when you destroy order then you have no alternatives. What you get is chaos. And I don't think that chaos is good for Israel or that chaos is good for, for the, for the. 

01:01:27 

Jews for the Jewish people. 

01:01:30 

And This is why I think that it will be very even from a very kind of transactional, very narrow perspective. You know, America is a long, long way from Israel, from the Middle East. 

01:01:43 

An isolationist America that withdraws from the world order is not good news for Israel. 

01:01:51 

So what do you say to the there are there? Are there are a number of American Jews who say, and I'm Jewish and I I this is not something that I say, but I hear it constantly. They say Trump would be good for the Jews. Trump would be good for Israel. He would protect Israel. 

01:02:06 

In in in what way? 

01:02:11 

He would have a open checkbook and send the military arms and everything that was asked no questions as. 

01:02:19 

Long as it serves his interests and if at a particular point it serves his interests to make a deal with Putin or with Iran or with anybody. 

01:02:30 

It is really expense. You will do it. 

01:02:33 

Fair enough. 

01:02:33 

I mean, he's not committed. 

01:02:38 

I asked the question to provoke an answer. Let me ask you. Let me ask you this, and this is a great segue back into the conversation we've been having the last hour you've written about. 

01:02:47 

Human how humans have evolved to pursue power. 

01:02:52 

And and knowledge but. 

01:02:53 

Power, I think, is the big focus. 

01:02:55 

How does the pursuit? 

01:02:57 

Of happiness fit into the narrative. 

01:03:00 

Hmm. 

01:03:01 

And I I don't think that humans are kind of obsessed only with power. I think that power is relatively superficial thing in the human condition. 

01:03:12 

It's a means to achieve various ends. It's it's not necessarily bad, it's not that power is always bad. No, it can be used for good. It can be used for bad. 

01:03:22 

But the deep motivation of humans, I think, is the pursuit of happiness and the pursuit of truth, which are related to one another, because you can never really be happy if you don't know the truth about yourself, about your life. 

01:03:39 

Unfortunately, as often happens, the means becomes an end. 

01:03:45 

That people become obsessed with power, not with what they can do with it. 

01:03:52 

That immense power, and they don't know what to do with it. All they do with it, very bad things. 

01:03:59 

This is this is an interesting one. What are some arguments for and against the future in which humans no longer have relationships with other humans and only have relationships with AI? 

01:04:14 

Hmm. 

01:04:17 

One thing to say about it is that AI is becoming better and better at understanding our feelings, our emotions, and therefore of developing relationships and intimate relationships with us. Because there is a deep yearning in human beings to be under. 

01:04:34 

Food we always want people to understand us, to understand how we feel. We want my husband, my parents, my teachers, my boss, to understand how I feel. And very often we are disappointed. They don't understand how I, how I feel, partly because they are too preoccupied with their own feelings to care about my feel. 

01:04:55 

Thanks. 

01:04:56 

AI's will not have this problem. They don't have any feelings of their own, and they can be 100% focused on deciphering, on analyzing your feelings. 

01:05:10 

So you know, in these all these science fiction movies in which the robots are extremely cold and mechanical and they can't understand the most basic human emotion. 

01:05:20 

It it's it's a complete opposite. 

01:05:23 

Part of the issue we are facing is that they will be so good at understanding human emotions and reacting in a way which is exactly calibrated to your personality at this particular moment that we might become exasperated with the human beings. 

01:05:44 

We don't have this capacity. 

01:05:47 

To understand our emotions and to react in in such. 

01:05:51 

A calibrated way. 

01:05:53 

There is a very big question which we didn't deal with and it it it's a long question of whether AI's will develop. 

01:06:00 

Emotions, feelings of their own, whether they become conscious or not. At present, we don't see any sign of it. But even if AI's don't develop any feelings of their own, once we become emotionally attached to them. 

01:06:16 

It is likely that we would start treating them as our conscious entities as sentient beings, and will confirm confirm them. The legal status of of persons you know in in the US there is actually a legal path already open for that. 

01:06:36 

Are corporations according to U.S. law, are legal persons. They have rights, their freedom of freedom of speech. For instance. Now you can incorporate an AI. 

01:06:47 

When you incorporate a corporation like Google or Facebook or whatever up and until today, this was to some extent just make believe because all the decisions of the corporations had to be made by human beings, by the executives, the lawyers, the accountants. 

01:07:06 

What happens if you incorporate an AI? It's not a legal person, and it can make decisions by itself. It doesn't need any human team to run it. So you start having legal persons, let's say in the US, which are not human. 

01:07:22 

And in many ways are more intelligent than us, and they can start making money, for instance, going on task rabbit and offering their services to various things like writing texts. So they earn money, and then they go to the to to, to, to the market they go to Wall Street and they invest that money. And because they're so intelligent. 

01:07:42 

Maybe they make billions and so you have a situation in which perhaps the richest person. 

01:07:51 

In the US, is not a human being, and part of their rights is that they have honest final speech, the right to make political contributions. 

01:08:02 

So this AI person can contribute billions of dollars to some candidate in exchange for getting more rights to AIS. 

01:08:04 

Now lobby is. Where is this going? 

01:08:15 

So there'll be an AI president. 

01:08:17 

I'm hoping you're going to leave. 

01:08:18 

Me. Leave all of us. 

01:08:20 

Maybe on a high note here with something optimistic. 

01:08:24 

Here's the question, and it's my final question. Humans are not always able to think from other perspectives. 

01:08:31 

Is AI able to think from multiple perspectives? I think the answer is yes, but do you think that AI will actually help us think this way? 

01:08:35 

Yeah. 

01:08:43 

That's one of the positive scenarios about AI that they will help us understand ourselves better. 

01:08:52 

That their immense power will be used not to manipulate us, but to help us. 

01:09:00 

And then we have historical precedents for that. 

01:09:04 

Like we have relationships with our humans, like doctors, like lawyers, accountants, therapists that are know a lot of things about us. 

01:09:16 

Some of, like our most private information, is held by these people and they have a fiduciary duty to use our private information and their expertise to help us. 

01:09:30 

And this is not. You don't need to invent the wheel, this is already there, and it's obvious that if they use it to manipulate us, or if they sell it to a third party to manipulate us, it's basically against the law. They can go to prison for that, and we should have the same thing with with AI. And we talked a lot about the dangers of AI. 

01:09:52 

But obviously AI has enormous positive potential, otherwise we would not be developing it. It could provide us with the best healthcare and history it could prevent both car accidents. 

01:10:04 

It it can also you know you can have an armies of AI doctors and teachers and therapists who help us. 

01:10:13 

Including help us understand our own humanity, our own relationships, our own emotions better. This can happen if we make the right decisions in the. 

01:10:26 

Next few years. 

01:10:27 

I would end maybe by saying that again the, the, the, the it's not that we lack the power. 

01:10:34 

At the present moment, what we lack is the understanding and the attention. 

01:10:39 

This is potentially the biggest technological revolution in history, and it's moving extremely fast, but that's the key problem. It's just moving extremely fast. If you think about the US elections the coming weeks, elections. So whoever wins the elections over the next four years, some of the most important decisions they will have to make. 

01:11:02 

Would be about AIS. 

01:11:04 

And regulating AI's and AI safety and and and and so forth. And it's not one of the main issues in the presidential debates. It's not even clear what is the difference, if there is one between Republicans and Democrats on AI, so on specific issues, we start seeing differences. 

01:11:24 

When it comes to issues of freedom of speech and regulation and so forth, but about the broader question, it's it's it's not clear at all. 

01:11:33 

And again, the the biggest danger of all is that we will just rush forward without thinking and without developing the mechanisms to to to, to slow down or to stop if necessary. 

01:11:49 

You know, if you think about it like like a car. So when they told me how to drive a car, the first thing I learned is how to use the brakes. 

01:11:59 

But the first thing that I think that teach most people. 

01:12:03 

Only after you know how to use the brakes, they teach you how to use the fuel pedal. They accelerate and it's the same when you learn how to ski. I never learned how to ski, but people who had told me the first thing they tell you is how to how to stop, how to fall. It's a bad idea to 1st teach you how to kind of, OK. 

01:12:16 

Hey, Janice. 

01:12:23 

Go faster and then when you are down the slope they start shouting. OK, this is. 

01:12:27 

How you stop? 

01:12:28 

And this is what we're doing with AI. 

01:12:31 

Like you have this cause of people in places like Silicon Valley, let's go as fast as we can if. 

01:12:37 

There is a problem down. 

01:12:38 

The road, we'll figure it out how to stop. 

01:12:41 

That's very, very dangerous. 

01:12:43 

You all before you go, let me ask you one final final question and it's news. We could all use you are writing this whole book about AI and technology and you do not carry a smartphone. 

01:12:57 

Is this true? 

01:12:58 

I have a kind of, you know, emergency smartphone because for various services, but I don't carry it with me, it's like. 

01:13:01 

How does your whole life work? Because. 

01:13:04 

I'm told you do not carry a phone. You don't have an e-mail the whole. 

01:13:07 

Thing. 

01:13:08 

No, I have e-mail. 

01:13:08 

I have e-mail. 

01:13:09 

I don't have and I try to use technology but not to be used by it. 

01:13:16 

And part of the answer is that they have a team who is carrying the smartphone and doing all that for me. 

01:13:25 

So it's not so fair to say that I don't, I don't have it. But I think on on a on a bigger issue, what we can say it's it's a bit like with food. 

01:13:36 

That 100 years ago, food was scarce, so people ate. 

01:13:42 

Whatever they could, and if they found something full of sugar and fat, they ate as much of it as possible because it gave you a lot of energy. 

01:13:52 

And now we are flooded by enormous amounts of food and junk food which is artificially pumped full of fat and sugar and is creating immense health problems. And most people have realized that more food is not always good for me and that I need to have some kind of diet. 

01:14:14 

And it's exactly the same with information. When the information died, a previously information was scarce, so we consumed whatever we could find. Now we are flooded by enormous amounts of information, and much of it is junk information which is artificially filled with hatred. 

01:14:34 

And greed and fear. And we basically need to grow on an information diet. 

01:14:41 

And consume less information. 

01:14:44 

And be far more mindful about what we put inside that information is the food for the mind. And you know, you feed your mind with unhealthy information. You'll have sick mind. It's as simple as that. 

01:14:59 

Well, we want to thank you, ball add Nexus to your information diet because it's an important document, our future and our world and want to thank you for this fascinating conversation. Thank you. And thank you for your questions. 

01:15:07 

About. 

01:15:11 

Thank you. 
